{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wg0d5jojmFFy"
   },
   "source": [
    " # Tutorial 09 - Neural Networks\n",
    " \n",
    " ## Dr. David C. Schedl\n",
    "\n",
    " Note: this tutorial is geared towards students **experienced in programming** and aims to introduce you to **Digital Imaging / Computer Vision** techniques.\n",
    "\n",
    "\n",
    "\n",
    "## Setup\n",
    "As first step, we need to import the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0Em9UTymFF1"
   },
   "outputs": [],
   "source": [
    "# Setup and import of libraries\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_blobs, make_circles\n",
    "\n",
    "\n",
    "# Let's set the random seed to make this reproducible (the same for everybody).\n",
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "W3ttSd1AmFF8"
   },
   "source": [
    "# Let's train an MLP to classify points (like Tensorflow Playground)\n",
    "\n",
    "We will at first generate some data using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "5Juvwq-QmFF8",
    "outputId": "1898ae0a-d731-4bbd-cd9c-50a922353fa7"
   },
   "outputs": [],
   "source": [
    "# make up a dataset\n",
    "X, y = make_moons(n_samples=1000, noise=0.1)\n",
    "\n",
    "y = y * 2 - 1  # make y be -1 or 1\n",
    "# visualize in 2D\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=\"jet\")\n",
    "plt.show()\n",
    "print(np.unique(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HXxKcOetmFF9"
   },
   "source": [
    "Let's set up an MLP in PyTorch. <br>\n",
    "\n",
    "Our model takes 2 inputs and outputs 1 value. <br>\n",
    "We will use 2 hidden layers with 16 neurons each. <br>\n",
    "Thus, our MLP has 337 parameters. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkpvoEY8mFF9",
    "outputId": "650c105e-79bd-4626-b508-ac827f97c31e"
   },
   "outputs": [],
   "source": [
    "# MLP in PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "%pip install torchinfo # external package to print model summary (like TensorFlow's model.summary())\n",
    "\n",
    "\n",
    "\n",
    "class TorchMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = torch.tanh(self.fc1(x))\n",
    "        h2 = torch.tanh(self.fc2(h1))\n",
    "        o = self.fc3(h2)\n",
    "        return o\n",
    "\n",
    "\n",
    "torch_model = TorchMLP()\n",
    "\n",
    "print(torch_model)\n",
    "# print number of parameters\n",
    "print(sum(p.numel() for p in torch_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kEw1_9iOmFF-",
    "outputId": "e55be6c3-6396-4fd4-a67b-b68517683e69"
   },
   "outputs": [],
   "source": [
    "# initial accuracy (RANDOM)\n",
    "preds = torch_model(torch.Tensor(X)).data.numpy()\n",
    "accuracy = [(yi > 0) == (scorei > 0) for yi, scorei in zip(y, preds)]\n",
    "acc = sum(accuracy) / len(accuracy)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4g10TFjHmFF-",
    "outputId": "011dd250-dc92-4388-ffb8-406193a333fc"
   },
   "outputs": [],
   "source": [
    "# soft marigin loss\n",
    "torch_loss_fun = F.soft_margin_loss\n",
    "optimizer = torch.optim.SGD(\n",
    "    torch_model.parameters(),\n",
    "    lr=5,  # learning rate\n",
    "    weight_decay=1e-4,  # L2 regularization\n",
    ")\n",
    "batch_size = None  # mini-batch size\n",
    "\n",
    "# optimization\n",
    "for k in range(100):\n",
    "\n",
    "    # use mini-batch\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y # entire dataset\n",
    "    else:\n",
    "        # use a random batch of the data\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    torch_scores = torch_model(torch.from_numpy(Xb).float())\n",
    "    torch_loss = torch_loss_fun(\n",
    "        torch_scores, torch.from_numpy(yb).reshape(-1, 1).float()\n",
    "    )\n",
    "    # L2 regularization is in the optimizer, now!!\n",
    "\n",
    "    # backward\n",
    "    torch_loss.backward()\n",
    "\n",
    "    # update (sgd)\n",
    "    optimizer.step()\n",
    "\n",
    "    # also get accuracy\n",
    "    accuracy = [\n",
    "        (yi > 0) == (scorei.data.item() > 0) for yi, scorei in zip(yb, torch_scores)\n",
    "    ]\n",
    "    acc = sum(accuracy) / len(accuracy)\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"step {k} loss {torch_loss.data}, accuracy {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "LcPTAE61mFF-",
    "outputId": "72981e83-8905-43f8-c704-e93422c22cee"
   },
   "outputs": [],
   "source": [
    "# visualize decision boundary (similar to tensorflow playground)\n",
    "\n",
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "scores = torch_model(torch.from_numpy(Xmesh).float()).data.numpy()\n",
    "Z = np.array([s > 0 for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RZFroauemFF-"
   },
   "source": [
    "### Exercise 01 üìù: Play with the data and the network\n",
    " \n",
    "Change the MLP (number of layers and number of neurons per layer) and see how it affects the decision boundary. \n",
    "Switch to the `make_circles` dataset (instead of the `make_moons` function) and see how your network performs. <br>\n",
    "\n",
    "Try to answer the following questions:\n",
    "- How many parameters do the networks have and how does that affect performance (accuracy and timing)?\n",
    "- What is the simplest and most complex network you can train to classify the data? Can it get too complex?\n",
    "- What happens if you change the batch size? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 with PyTorch\n",
    "\n",
    "Let's try to classify the CIFAR10 dataset. We can use the torchvision package to load the CIFAR10 dataset. \n",
    "After loading the dataset, we'll need to preprocess the images by reshaping them to a 1D tensor and normalizing the pixel values. The `transform` takes care of this.\n",
    "\n",
    "Afterwards let's display some images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# Load and preprocess the CIFAR10 dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# the labels will be put in a separate vector as the original is just numbers, but we want text labels \n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# display some images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show 10 images\n",
    "plt.title('|'.join('% 5s' % classes[labels[j].item()] for j in range(10)))\n",
    "imshow(torchvision.utils.make_grid(images[:10], nrow=10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's define our model. We use PyTorch's nn module to define the architecture of the model. The input layer should have 3072 neurons ($32\\times32\\times3$), and the output layer should have 10 neurons, one for each class. \n",
    "\n",
    "Later you will change this model to an MLP. The number of neurons in the hidden layers, as well as the number of hidden layers, is up to you!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the (linear) model\n",
    "class CIFAR10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10, self).__init__()\n",
    "        self.linear = nn.Linear(3072, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3072)\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# summary of the model\n",
    "from torchinfo import summary\n",
    "linear_model = CIFAR10()\n",
    "print(summary(linear_model, inputs.shape))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For training we define the loss function and optimizer. Let's use the cross-entropy loss function and a stochastic gradient descent (SGD) or Adam optimizer to train the model.\n",
    "\n",
    "Then we need to loop over the training dataset, feed the images and labels to the model, compute the loss, perform backpropagation to update the model's parameters, and repeat for a certain number of epochs.\n",
    "\n",
    "Afterwards we can test the model on the test dataset. We can use the `torch.no_grad()` context manager to temporarily set all the requires_grad flag to false. This will reduce memory usage and speed up computations. We don't need to compute gradients in the testing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CIFAR10()\n",
    "model = CNNModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # keep loss for statistics\n",
    "        running_loss += loss.item()\n",
    "    # print statistics\n",
    "    print('Epoch %d loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Test the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few images from the test set and print the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random test images\n",
    "images, labels = next(iter(testloader))\n",
    "\n",
    "\n",
    "# set up a figure\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "_, prediction_label = torch.max(model(images).data, 1)\n",
    "\n",
    "total, correct = 0, 0\n",
    "# plot the images: each image is 28x28 pixels\n",
    "for i,img in enumerate(images[:50]):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    ax = fig.add_subplot(5, 10, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n",
    "\n",
    "\n",
    "    img_text = f'{classes[prediction_label[i]]} [{classes[labels[i]]}]'\n",
    "\n",
    "    if prediction_label[i] == labels[i]:\n",
    "        # label the image with the blue text\n",
    "        ax.text(0.1, 0.1, img_text, color='lightgreen', transform=ax.transAxes)\n",
    "        ax.tick_params(color='green', labelcolor='green')\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor('green')\n",
    "        correct += 1\n",
    "    else:\n",
    "        # label the image with the red text\n",
    "        ax.text(0.1, 0.1, img_text, color='darkred', transform=ax.transAxes)\n",
    "        ax.tick_params(color='red', labelcolor='red')\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor('red')\n",
    "    total += 1\n",
    "\n",
    "print(f'Accuracy: {correct/total*100:.2f}% for {total} test images')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the templates that the network has learned. We can access the weights of the linear layer of the network and display them as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpret the weights of the linear model\n",
    "\n",
    "if isinstance(model, CIFAR10):\n",
    "\n",
    "    plt.figure(figsize=(20,5))\n",
    "    for i,c in enumerate(classes):\n",
    "        bias = model.linear.bias[i].detach().numpy() # get the bias for the class\n",
    "        template = model.linear.weight[i].detach().numpy().reshape((3,32,32)) # get the weights for the class\n",
    "        template = template / 2 + 0.5     # unnormalize\n",
    "        \n",
    "        # flip the dimensions to get the correct image\n",
    "        template = np.transpose(template, (1,2,0))\n",
    "        template -= np.min(template)\n",
    "        template /= np.max(template)\n",
    "\n",
    "        plt.subplot(1,len(classes),i+1)\n",
    "        plt.imshow(template)\n",
    "        plt.title(c)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 02 üìù: Use an MLP to classify the CIFAR10 dataset\n",
    "\n",
    "Modify the model to classify the CIFAR-10 dataset with an MLP. <br>\n",
    "In that notebook there is a `CIFAR10` class that defines a PyTorch Module already: \n",
    "```python\n",
    "class CIFAR10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(...)\n",
    "        ...\n",
    "```\n",
    "\n",
    "Modify or copy the `CIFAR10` class and change it to an MLP. The number of neurons and layers is up to you! <br>\n",
    "Do you expect better or worse performance than with the linear classifier? <br>\n",
    "Report the accuracy of your new network and compare it to the linear model. <br>\n",
    "\n",
    "Note: Training a larger MLP with loads of data will take a while. So choose your hyperparameters wisely! :)\n",
    "\n",
    "Furthermore, it's worth noting that this MLP is not expected to perform well with CIFAR10 dataset as it's a more complex dataset compared to MNIST and MLP model does not have enough capacity to learn a good representation of this data, for this purpose convolutional neural network (CNN) is a better approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN (LeNe-5) to classify CIFAR-10\n",
    "\n",
    "\n",
    "Below you can find the code for a (modernized) LeNet-5 architecture in PyTorch. Inspired by [this](https://towardsdatascience.com/implementing-yann-lecuns-lenet-5-in-pytorch-5e05a0911320/) blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a model\n",
    "inputs, classes = next(iter(trainloader))\n",
    "input_shape = inputs[0].shape\n",
    "print(\"input:\", input_shape)\n",
    "nb_classes = 10\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_shape=[3, 32, 32], nb_classes=10, legacy=True):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.act = nn.Sigmoid() if legacy else nn.ReLU()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 6 if legacy else 20, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2),)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(6 if legacy else 20, 16 if legacy else 50 , kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_shape[1]//4*input_shape[2]//4*(16 if legacy else 50), 500)\n",
    "        self.fc2 = nn.Linear(500, nb_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "cnn_model = CNNModel(input_shape, nb_classes, legacy=True) # instance the model\n",
    "print( \"output:\", cnn_model(inputs).shape ) # check the output shape of the model -> (batch_size, nb_classes)\n",
    "\n",
    "# summary of the model\n",
    "from torchinfo import summary\n",
    "print(summary(cnn_model, inputs.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

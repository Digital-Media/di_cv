{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU831zdvOJ86"
      },
      "source": [
        "# Tutorial 05 - Features\n",
        "\n",
        "## Dr. David C. Schedl\n",
        "\n",
        "Note: this tutorial is geared towards students **experienced in programming** and aims to introduce you to **Computer Vision** techniques.\n",
        "\n",
        "\n",
        "Useful links:\n",
        "* OpenCV Tutorials: https://docs.opencv.org/master/d9/df8/tutorial_root.html\n",
        "* Image Processing in Pyhton: https://github.com/xn2333/OpenCV/blob/master/Seminar_Image_Processing_in_Python.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFUPuyJVOJ88"
      },
      "source": [
        "# Table of Contents  \n",
        "\n",
        "- Initialization\n",
        "- Harris Corner Detector\n",
        "- Histograms\n",
        "- SIFT Descriptor\n",
        "- Feature Matching\n",
        "- Warping\n",
        "- Everything Above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkE2YAvyOJ88"
      },
      "source": [
        "<a id=\"Initialization\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weMMc2WZlsJc"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkUzZ6lWK_S5"
      },
      "source": [
        "As always let's import useful libraries, first. \n",
        "We will work with images today. So let's download some with `curl`.\n",
        "Let's define utility functions to display images, in Jupyter Notebooks. OpenCV's `imshow` does not work and matplotlib's `imshow` needs special treatment due to color channel handling (RGB vs. BGR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtUm6wJFOJ89",
        "outputId": "651bf071-d5a9-4a08-cb78-7756c1e95252"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  # install a newer opencv version on Colab. The default does not support SIFT!\n",
        "  !pip install opencv-contrib-python==4.4.*\n",
        "\n",
        "# import the libraries we use\n",
        "import os\n",
        "import cv2 # openCV\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "# loading images\n",
        "!curl -o \"woman.jpg\" \"https://live.staticflickr.com/8859/18045025168_3a1ffa6521_c_d.jpg\"\n",
        "!curl -o \"foto1A.jpg\" \"https://www.ic.unicamp.br/~helio/imagens_registro/foto1A.jpg\"\n",
        "!curl -o \"foto1B.jpg\" \"https://www.ic.unicamp.br/~helio/imagens_registro/foto1B.jpg\"\n",
        "\n",
        "# utility function(s)\n",
        "def imshow(image, *args, **kwargs):\n",
        "    \"\"\"A replacement for cv2.imshow() for use in Jupyter notebooks using matplotlib.\n",
        "\n",
        "        Args:\n",
        "          image : np.ndarray. shape (N, M) or (N, M, 1) is an NxM grayscale image. shape\n",
        "            (N, M, 3) is an NxM BGR color image. \n",
        "    \"\"\"\n",
        "    if len(image.shape) == 3:\n",
        "      # Height, width, channels\n",
        "      # Assume BGR, do a conversion  \n",
        "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Draw the image\n",
        "    plt.imshow(image, *args, **kwargs)\n",
        "    # We'll also disable drawing the axes and tick marks in the plot, since it's actually an image\n",
        "    plt.axis('off')\n",
        "    # Make sure it outputs\n",
        "    # plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cYcrUMAOJ89"
      },
      "source": [
        "# Harris Corner Detector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBLJLWY1OJ8-"
      },
      "source": [
        "## OpenCV's Harris implementation\n",
        "\n",
        "Let's first look at the implementation available with OpenCV. \n",
        "```\n",
        "cv2.cornerHarris(src, blockSize, ksize, k)\n",
        "```\n",
        "The `cornerHarris` function takes an image, the window size, a size parameter for the Sobel operator and k, the Harris detector free parameter, as input.\n",
        "Let's apply it to our example image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "TzxVHCckOJ8-",
        "outputId": "b5fbbdad-1beb-4586-d124-9b580c23c255"
      },
      "outputs": [],
      "source": [
        "filename = \"woman.jpg\"\n",
        "img = cv2.imread(filename)\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "dst = cv2.cornerHarris(gray.astype(np.float32),5,3,0.04)\n",
        "\n",
        "#result is dilated for marking the corners, not important\n",
        "dst = cv2.dilate(dst,None)\n",
        "\n",
        "# Threshold for an optimal value, it may vary depending on the image.\n",
        "img[dst>0.01*dst.max()]=[0,0,255]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "imshow(img)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9ihwj--OJ8-"
      },
      "source": [
        "## Non-OpenCV implementation\n",
        "\n",
        "To understand the algorithm better let's also look at an implementation without the builtin function of OpenCV. This implementation can also be found [online](https://github.com/hughesj919/HarrisCorner/blob/master/Corners.py). \n",
        "Note: the implementation is not very optimized and just for illustration. \n",
        "A local maximum suppression of R is not used.\n",
        "Furthermore, it allows us to look at the computed R. \n",
        "Note that this implementation is using the alternative equation to compute the corner response $R$:\n",
        "$$\n",
        "R = \\text{det}(M) - \\alpha \\thinspace \\text{trace}(M)^2 = \\lambda_1 \\lambda_2 - \\alpha (\\lambda_1 + \\lambda_2)^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "YfdDXZXXOJ8-",
        "outputId": "62b3e121-b8f0-4c89-f3f6-6d4ed1c6964c"
      },
      "outputs": [],
      "source": [
        "def findCorners(img, window_size=5, k=0.04, thresh=10000 ):\n",
        "    \"\"\"\n",
        "    Finds and returns list of corners and new image with corners drawn\n",
        "    :param img: The original image\n",
        "    :param window_size: The size (side length) of the sliding window\n",
        "    :param k: Harris corner constant. Usually 0.04 - 0.06\n",
        "    :param thresh: The threshold above which a corner is counted\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    #Find x and y derivatives\n",
        "    dy, dx = np.gradient(img)\n",
        "    Ixx = dx**2\n",
        "    Ixy = dy*dx\n",
        "    Iyy = dy**2\n",
        "    height = img.shape[0]\n",
        "    width = img.shape[1]\n",
        "\n",
        "    cornerList = []\n",
        "    newImg = img.copy()\n",
        "    color_img = cv2.cvtColor(newImg, cv2.COLOR_GRAY2RGB)\n",
        "    offset = int(window_size/2)\n",
        "    R = np.zeros_like(dx)\n",
        "\n",
        "    #Loop through image and find our corners\n",
        "    for y in range(offset, height-offset):\n",
        "        for x in range(offset, width-offset):\n",
        "            #Calculate sum of squares\n",
        "            windowIxx = Ixx[y-offset:y+offset+1, x-offset:x+offset+1]\n",
        "            windowIxy = Ixy[y-offset:y+offset+1, x-offset:x+offset+1]\n",
        "            windowIyy = Iyy[y-offset:y+offset+1, x-offset:x+offset+1]\n",
        "            Sxx = windowIxx.sum()\n",
        "            Sxy = windowIxy.sum()\n",
        "            Syy = windowIyy.sum()\n",
        "\n",
        "            #Find determinant and trace, use to get corner response\n",
        "            det = (Sxx * Syy) - (Sxy**2)\n",
        "            trace = Sxx + Syy\n",
        "            r = det - k*(trace**2)\n",
        "\n",
        "            R[y,x] = r # store in R matrix\n",
        "\n",
        "            #If corner response is over threshold, color the point and add to corner list\n",
        "            if r > thresh:\n",
        "                #print x, y, r\n",
        "                cornerList.append([x, y, r])\n",
        "                color_img.itemset((y, x, 0), 0)\n",
        "                color_img.itemset((y, x, 1), 0)\n",
        "                color_img.itemset((y, x, 2), 255)\n",
        "    return color_img, R, cornerList\n",
        "\n",
        "\n",
        "finalImg, R, cornerList = findCorners(gray, 5, 0.05, thresh=10000000)\n",
        "\n",
        "# plotting\n",
        "plt.figure(figsize=(20,15))\n",
        "plt.subplot(131), imshow(gray, cmap='gray'), plt.title( 'original' )\n",
        "response = R.copy()\n",
        "response[R<0]=0 # remove negative values\n",
        "plt.subplot(132), imshow(np.log(response+1.0), cmap='hot'), plt.title( 'corner response $R$' )\n",
        "plt.subplot(133),imshow(finalImg), plt.title( 'Harris corners' ) \n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMmZzm_oOJ8_"
      },
      "source": [
        "# Histograms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNCIWj7vQaL6"
      },
      "source": [
        "## OpenCV's image histogram computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "XsYYsFDoOJ8_",
        "outputId": "3e8cccd8-af1a-4ce7-9f4b-93053e87b1ba"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread('foto1A.jpg')\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) # convert to grayscale\n",
        "\n",
        "color = ('b','g','r')\n",
        "plt.figure(figsize=(15,5)) # this command makes the figure larger so we see the images better\n",
        "plt.subplot(131), imshow(img, cmap=\"gray\"), plt.title('color image')\n",
        "\n",
        "# grayscale\n",
        "plt.subplot(132), plt.title('grayscale histogram')\n",
        "histr = cv2.calcHist([gray],[0],None,[256],[0,256])\n",
        "plt.plot(histr,color = 'gray')\n",
        "\n",
        "# RGB\n",
        "plt.subplot(133), plt.title('color histogram')\n",
        "for i,col in enumerate(color):\n",
        "    histr = cv2.calcHist([img],[i],None,[256],[0,256])\n",
        "    plt.plot(histr,color = col)\n",
        "    plt.xlim([0,256])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpEMn9ZuOJ9A"
      },
      "source": [
        "## âŒ¨ï¸ Try it yourself: Implement your own (grayscale) histogram function\n",
        "\n",
        "Program an image histogram yourself. An 8-bit image can have 256 different values (0 to 255), use them as histogram bins.\n",
        "\n",
        "(Python) Challenge(s): \n",
        " - Can you do it in one line / as short as possible?\n",
        " - What happens if you modify the bin sizes?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "D-EaDJj9OJ9A",
        "outputId": "820cf896-1593-4ebd-b9dc-3b87f42c6c80"
      },
      "outputs": [],
      "source": [
        "def compute_hist( gray ):\n",
        "    assert len(gray.shape)==2 # make sure it is 2D and grayscale\n",
        "    bins = np.zeros((256))\n",
        "\n",
        "    # Todo\n",
        "\n",
        "    return bins\n",
        "\n",
        "\n",
        "# test it!\n",
        "plt.figure(figsize=(15,5)) # this command makes the figure larger so we see the images better\n",
        "plt.subplot(121), imshow(gray, cmap=\"gray\"), plt.title('grayscale image')\n",
        "plt.subplot(122), plt.title('my histogram')\n",
        "my_histr = compute_hist(gray)\n",
        "plt.plot(my_histr,color = 'gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHUusFMtOJ9A"
      },
      "source": [
        "# SIFT Descriptor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7ekT4WpigMC"
      },
      "source": [
        "## OpenCV's SIFT implementation\n",
        "\n",
        "Make sure that you have a recent OpenCV version installed. SIFT was patented and is only free since 2020.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "5Qm_NcD0E9gC",
        "outputId": "b11ea554-7731-4418-d44c-d01e41d89f81"
      },
      "outputs": [],
      "source": [
        "# read images and transform them to grayscale\n",
        "# Make sure that the train image is the image that will be transformed\n",
        "\n",
        "imgA = cv2.imread(\"foto1A.jpg\")\n",
        "imgB = cv2.imread(\"foto1B.jpg\")\n",
        "\n",
        "# convert to grayscale\n",
        "imgA_gray = cv2.cvtColor(imgA, cv2.COLOR_RGB2GRAY)\n",
        "imgB_gray = cv2.cvtColor(imgB, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "\n",
        "# display\n",
        "plt.figure(figsize=(15,10)) # this command makes the figure larger so we see the images better\n",
        "plt.subplot(121), imshow(imgA, cmap=\"gray\"), plt.title('image A')\n",
        "plt.subplot(122), imshow(imgB, cmap=\"gray\"), plt.title('image B')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "Mb0_FCAIE9gO",
        "outputId": "41e9cb04-ec3c-430d-de5f-6da01b4a635d"
      },
      "outputs": [],
      "source": [
        "# init sift descriptor\n",
        "sift = cv2.SIFT_create()\n",
        "\n",
        "     \n",
        "# get keypoints and descriptors\n",
        "(kpsA, featuresA) = sift.detectAndCompute(imgA_gray, None)\n",
        "print(featuresA.shape)\n",
        "\n",
        "# display the keypoints\n",
        "imgA_sift=cv2.drawKeypoints(imgA_gray,kpsA,imgA_gray.copy() ,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        "\n",
        "# display\n",
        "plt.figure(figsize=(15,10)) # this command makes the figure larger so we see more details\n",
        "#plt.subplot(121), imshow(imgA, cmap=\"gray\"), plt.title('image 1')\n",
        "imshow(imgA_sift, cmap=\"gray\"), plt.title('SIFT keypoints for image A ({} points)'.format(featuresA.shape[0]))\n",
        "plt.show()\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34QcCnldmUK5"
      },
      "source": [
        "Every interest point is descriped by a 128-dimensional feature vector. \n",
        "Let's display some as rows in an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "DYI4SVXal5YT",
        "outputId": "ba63fae2-3007-429f-99c8-4e3742e8b27d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,10)) # this command makes the figure larger so we see the images better\n",
        "imshow(featuresA[:16]), plt.title('SIFT descriptors (128 components; every row is a descriptor)')\n",
        "plt.show()\n",
        "#print(featuresA[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKvtAneejAp7"
      },
      "source": [
        "Furthermore, openCV provides implementations for other descriptors such as SURF, ORB, and BRISK. \n",
        "You only need to replace `cv2.SIFT_create()` with another descriptor like `cv2.SURF_create()` or `cv2.BRISK_create()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmOO4F46idGz"
      },
      "source": [
        "# Feature Matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2I9DhWsjgKc"
      },
      "source": [
        "Determine correspondence between interest points in two views by matching the feature descriptors. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "wfqfQCIuE9gU",
        "outputId": "91c13144-15b6-4830-f379-7acd3f95b727"
      },
      "outputs": [],
      "source": [
        "(kpsB, featuresB) = sift.detectAndCompute(imgB_gray, None) # compute features for second image\n",
        "\n",
        "# display the keypoints and features detected on both images\n",
        "plt.figure(figsize=(20,10)) # this command makes the figure larger so we see the images better\n",
        "plt.subplot(121), imshow(cv2.drawKeypoints(imgA_gray,kpsA,None,color=(0,255,0)), cmap=\"gray\"), plt.title('image A')\n",
        "plt.subplot(122), imshow(cv2.drawKeypoints(imgB_gray,kpsB,None,color=(0,255,0)), cmap=\"gray\"), plt.title('image B')\n",
        "plt.show()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S95R6bdcmJSj"
      },
      "source": [
        "## Simple Nearest Neighbour Matches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "YIoACRVHE9ga",
        "outputId": "0f6592a5-49a8-4dcd-eb0c-c3ce1cc7f735"
      },
      "outputs": [],
      "source": [
        "# Simple Nearest Neighbour Matches\n",
        "\n",
        "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True) # init OpenCV's matcher\n",
        "    \n",
        "# Match descriptors.\n",
        "best_matches = bf.match(featuresB,featuresA)\n",
        "\n",
        "# Sort the features in order of distance.\n",
        "# The points with small distance (more similarity) are ordered first in the vector\n",
        "matches = sorted(best_matches, key = lambda x:x.distance)\n",
        "print(\"#matches (simple):\", len(matches))\n",
        "print(\"match[0]: \", matches[0].distance, matches[0].imgIdx, matches[0].queryIdx, matches[0].trainIdx)\n",
        "print(\"match[100]: \", matches[100].distance, matches[100].imgIdx, matches[100].queryIdx, matches[100].trainIdx)\n",
        "\n",
        "# display (some) matches\n",
        "img_matches = cv2.drawMatches(imgB_gray,kpsB,imgA_gray,kpsA,matches[:1000:10],\n",
        "                           None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "plt.figure(figsize=(20,10)) # this command makes the figure larger so we see the images better\n",
        "imshow(img_matches)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_cwqP2WzPgO"
      },
      "source": [
        "Let's look at the offsets of matched features and let's plot them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "CmhucNvhml7o",
        "outputId": "9d7c41d0-17cf-4eb6-8ae5-a0ebc1419c25"
      },
      "outputs": [],
      "source": [
        "def getMatchedPoints(kpsA, kpsB, featuresA, featuresB, matches):\n",
        "    # convert the keypoints to numpy arrays\n",
        "    kpsA = np.float32([kp.pt for kp in kpsA])\n",
        "    kpsB = np.float32([kp.pt for kp in kpsB])\n",
        "    \n",
        "    if len(matches) > 4:\n",
        "\n",
        "        # construct the two sets of points\n",
        "        ptsA = np.float32([kpsA[m.queryIdx] for m in matches])\n",
        "        ptsB = np.float32([kpsB[m.trainIdx] for m in matches])\n",
        "       \n",
        "        return (ptsA, ptsB)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "(ptsA, ptsB) =    getMatchedPoints(kpsB, kpsA, featuresB, featuresA, matches[:])\n",
        "\n",
        "diff = ptsA - ptsB # compute offsets between matched points\n",
        "simDistance = np.float32([m.distance for m in matches])\n",
        "\n",
        "# display\n",
        "fig = plt.figure(figsize=(15,10)) # this command makes the figure larger so we see the images better\n",
        "ax = fig.add_subplot(111)\n",
        "plt.scatter(diff[:,0],diff[:,1],c=simDistance,cmap='viridis')\n",
        "ax.set_aspect('equal', adjustable='box'), plt.xlabel('$\\Delta x$'), plt.ylabel('$\\Delta y$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds2BaYIemNa4"
      },
      "source": [
        " ## Nearest Neighbor Distance Ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "D2huykCrE9gd",
        "outputId": "babc66f0-462d-4af6-a38f-05afd5a8aac8"
      },
      "outputs": [],
      "source": [
        "# Nearest Neigbhor Distance Ratio\n",
        "ratio = 0.5 # optionally use .75\n",
        "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=False) # init OpenCV's matcher\n",
        "\n",
        "rawMatches = bf.knnMatch(featuresB, featuresA, 2) # retrieves the two nearest neighbours\n",
        "matches = []\n",
        "\n",
        "# loop over the raw matches\n",
        "for m,n in rawMatches:\n",
        "    # ensure the distance is within a certain ratio of each\n",
        "    # other (i.e. Lowe's ratio test)\n",
        "    if m.distance < n.distance * ratio:\n",
        "        matches.append(m)\n",
        "\n",
        "# sort based on the distance of the closest neighbour\n",
        "matches = sorted(matches, key = lambda x:x.distance)\n",
        "\n",
        "print(\"#Matches (KNN Distance Ratio):\", len(matches))\n",
        "print(\"match[0]: \", matches[0].distance, matches[0].imgIdx, matches[0].queryIdx, matches[0].trainIdx)\n",
        "print(\"match[100]: \", matches[100].distance, matches[100].imgIdx, matches[100].queryIdx, matches[100].trainIdx)\n",
        "\n",
        "# display (some) matches\n",
        "img_matches = cv2.drawMatches(imgB_gray,kpsB,imgA_gray,kpsA,matches[:1000:10],\n",
        "                           None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
        "plt.figure(figsize=(20,10)) # this command makes the figure larger so we see the images better\n",
        "imshow(img_matches)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRRWHPy7zZBe"
      },
      "source": [
        "Let's look at the offsets of matched features again. \n",
        "This has improved with the NN distance ratio matching!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "t0cMy9f7oYUV",
        "outputId": "5846125d-a2a2-4f48-e088-e33e4aa91cd3"
      },
      "outputs": [],
      "source": [
        "# plot the Î”x and Î”y of matches points\n",
        "(ptsA, ptsB) =    getMatchedPoints(kpsB, kpsA, featuresB, featuresA, matches[:])\n",
        "\n",
        "diff = ptsA - ptsB # compute offsets between matched points\n",
        "simDistance = np.float32([m.distance for m in matches])\n",
        "\n",
        "# display\n",
        "fig = plt.figure(figsize=(15,10)) # this command makes the figure larger so we see the images better\n",
        "ax = fig.add_subplot(111)\n",
        "plt.scatter(diff[:,0],diff[:,1],c=simDistance,cmap='viridis')\n",
        "ax.set_aspect('equal', adjustable='box'), plt.xlabel('$\\Delta x$'), plt.ylabel('$\\Delta y$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unvCSi28zgZv"
      },
      "source": [
        "# Warping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxm7YrB6zj4_"
      },
      "source": [
        "## Simple Translational Model\n",
        "\n",
        "Let's merge the two images by translating/warping one image (image A). \n",
        "So we assume that a point $\\bf x$ in one image corresponds to $\\bf x' = \\bf x + \\bf t$ in the second image, where $\\bf t$ is the translation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        },
        "id": "IgBHhSxsE9go",
        "outputId": "c4e3606b-721c-4633-f203-bec000a29ac6"
      },
      "outputs": [],
      "source": [
        "num_best_matches = int(len(matches)/2) # only use the best matches\n",
        "\n",
        "# estimate the image translation with the offset of matched points\n",
        "shiftX, shiftY = np.median(diff[:num_best_matches,0]), np.median(diff[:num_best_matches,1])\n",
        "H = np.float32([\n",
        "\t[1, 0, shiftX],\n",
        "\t[0, 1, shiftY]\n",
        "])\n",
        "print(H) # print the translation matrix\n",
        "\n",
        "# make sure the new image is large enough\n",
        "width  = imgA.shape[1] + int(imgB.shape[1]/2)\n",
        "height = imgA.shape[0] + int(imgB.shape[0]/3)\n",
        "\n",
        "# warp image A to image B\n",
        "result = cv2.warpAffine(imgA, H, (width, height))\n",
        "result[0:imgB.shape[0], 0:imgB.shape[1]] = imgB\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "imshow(result)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Affine Transformation Model\n",
        "\n",
        "As shown above, the assumption that a simple translation is enough to warp image A correctly onto B is not correct. The edges do not perfectly match because of this. \n",
        "Therefore, let's try to estimate an affine transformation matrix (which has 6 degrees of freedom), which handles a variety of transformations. \n",
        "However, it is still not perfect!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# estimate the homography between the sets of points\n",
        "(H, status) = cv2.estimateAffine2D(ptsB, ptsA, cv2.RANSAC)\n",
        "print(H) # the matrix is 3x2\n",
        "\n",
        "# make sure the new image is large enough\n",
        "width  = imgA.shape[1] + int(imgB.shape[1]*2/3)\n",
        "height = imgA.shape[0] + int(imgB.shape[0]/3)\n",
        "\n",
        "# warp image A to image B\n",
        "result = cv2.warpAffine(imgA, H, (width, height))\n",
        "result[0:imgB.shape[0], 0:imgB.shape[1]] = imgB\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "imshow(result)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDInDV5y4Dwt"
      },
      "source": [
        "## Perspective Transformation Model\n",
        "\n",
        "As shown above, a simple translation and even an affine transformation are not enough to warp image A correctly onto B. The edges do not perfectly match because of this. \n",
        "Therefore, let's try to estimate a perspective transformation matrix (which has 8 degrees of freedom), which handles a variety of transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "dMQ2PjJp4Qru",
        "outputId": "ccaf987f-9e56-4d48-f661-0a2265250f38"
      },
      "outputs": [],
      "source": [
        "# estimate the homography between the sets of points\n",
        "(H, status) = cv2.findHomography(ptsB, ptsA, cv2.RANSAC)\n",
        "print(H) # the matrix is 3x3 now\n",
        "\n",
        "# make sure the new image is large enough\n",
        "width  = imgA.shape[1] + int(imgB.shape[1]*2/3)\n",
        "height = imgA.shape[0] + int(imgB.shape[0]/3)\n",
        "\n",
        "# warp image A to image B\n",
        "result = cv2.warpPerspective(imgA, H, (width, height))\n",
        "result[0:imgB.shape[0], 0:imgB.shape[1]] = imgB\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "imshow(result)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N79omsa30GPa"
      },
      "source": [
        "# Everything above (in one line): OpenCV's Image Stitcher \n",
        "\n",
        "We could have avoided all the steps above by simply using OpenCV's image stitcher. It does something similar like what we did above and also smoothly blends images. But where is the fun in that? ðŸ˜ƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "1YvbfVdJpcqo",
        "outputId": "f493165e-2a59-433d-e996-ec6cb73c15f6"
      },
      "outputs": [],
      "source": [
        "# initialize OpenCV's image stitcher object and then perform the image\n",
        "# stitching\n",
        "images = [imgA, imgB]\n",
        "stitcher = cv2.Stitcher_create( mode=1 ) # mode=0 is panorama and assumes spherical stitichg, mode=1 is scan\n",
        "(status, stitched) = stitcher.stitch(images)\n",
        "\n",
        "if status==0: # stitching worked!\n",
        "  plt.figure(figsize=(20,10))\n",
        "  imshow(stitched)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "05_Features.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "0bae4c9257a18836fb2e3dc2d0aeb6355625d596c4075009294ab101cd3e0d3c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

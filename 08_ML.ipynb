{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wg0d5jojmFFy"
   },
   "source": [
    " # Tutorial 08 - Machine Learning\n",
    " \n",
    " ## Dr. David C. Schedl\n",
    "\n",
    " Note: this tutorial is geared towards students **experienced in programming** and aims to introduce you to **Digital Imaging / Computer Vision** techniques.\n",
    "\n",
    "\n",
    "\n",
    "## Setup\n",
    "As first step, we need to import the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0Em9UTymFF1"
   },
   "outputs": [],
   "source": [
    "# Setup and import of libraries\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_blobs, make_circles\n",
    "\n",
    "\n",
    "# Let's set the random seed to make this reproducible (the same for everybody).\n",
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lgQPWhSmGUZ"
   },
   "source": [
    "# Computing Gradients\n",
    "Let's start simple with the quadratic function $f(x) = 3x^2 - 4x + 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCDAFpYUmGUa"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3 * x**2 - 4 * x + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RgImgMGKmGUa"
   },
   "source": [
    "Let's plot it in the range $[-5, 5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "nqQ9SCzRmGUb",
    "outputId": "9eb5c5c8-0858-4437-b7fd-ce905cdf109a"
   },
   "outputs": [],
   "source": [
    "xs = np.arange(-5, 5, 0.25)\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys, label=\"f(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-UZ6M2BmGUb"
   },
   "source": [
    "The derivative of this function is $\\frac{df(x)}{dx} = 6x - 4$.\n",
    "\n",
    "The minimum ($0 = \\frac{df(x)}{dx}$) is at $x = \\frac{4}{6} = \\frac{2}{3}$.\n",
    "\n",
    "Let's numerically derive the function and let's look at some values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HvzWb-qumGUc",
    "outputId": "e94bf276-106c-43ef-a4ab-e7dcfaabd32e"
   },
   "outputs": [],
   "source": [
    "h = 0.000001\n",
    "x = -4  # test with -4, 0, 2/3 and 4\n",
    "(f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTxLj2xYmGUd"
   },
   "source": [
    "## Simple Gradient Descent\n",
    "\n",
    "With the gradient we can now implement a simple gradient descent algorithm, which iteratively updates the value of $x$ in the direction of the negative gradient.\n",
    "The parameters are the learning rate (often denoted as $\\alpha$) and the number of iterations $N$.\n",
    "\n",
    "🤔 What happens if you change the learning rate or the number of iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "xp80t_iGmGUe",
    "outputId": "fcf1e261-d77f-42e3-ad97-b248c548c908"
   },
   "outputs": [],
   "source": [
    "# simple gradient descent\n",
    "\n",
    "x = -4\n",
    "_xs = [x]\n",
    "learning_rate = 0.05\n",
    "N = 10  # number of iterations\n",
    "for i in range(N):\n",
    "    df = (f(x + h) - f(x)) / h\n",
    "    x = x - learning_rate * df\n",
    "    _xs.append(x)\n",
    "\n",
    "print(f\"x' reached: {x:.3f} after {N} iterations and should be {2/3:.3f}!\")\n",
    "plt.plot(xs, ys, label=\"f(x)\")  # plot the function\n",
    "plt.plot(\n",
    "    _xs, f(np.array(_xs)), \"r.\", label=\"x'\"\n",
    ")  # plot the path our gradient descent took\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqKm_PnwHlt1"
   },
   "source": [
    "## The same with PyTorch\n",
    "\n",
    "Let's reuse the quadratic function $f(x) = 3x^2 - 4x + 5$.\n",
    "\n",
    "PyTorch implements backpropagation. \n",
    "After calling `backward` every tensor involved in the computation has a gradient (`.grad`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfvndxmuHneV",
    "outputId": "cac71d54-8568-4f3c-f16b-3dc7745b656b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# test with -4, 0, 2/3 and 4\n",
    "x = torch.Tensor([-4]).double()\n",
    "x.requires_grad = True\n",
    "\n",
    "# the quadratic function\n",
    "f = 3 * x**2 - 4 * x + 5\n",
    "\n",
    "print(\"f =\", f.data.item())\n",
    "\n",
    "f.backward()  # with backward we compute the gradients\n",
    "\n",
    "print(\"---\")\n",
    "print(\"gradient x =\", x.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzcDjwu6MNZi",
    "outputId": "bd0973de-4d53-4c07-d989-7257d0e7fbd2"
   },
   "outputs": [],
   "source": [
    "x = torch.Tensor([-4]).double()\n",
    "x.requires_grad = True\n",
    "\n",
    "\n",
    "# Let's use an optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    [x],\n",
    "    lr=learning_rate,  # learning rate\n",
    ")\n",
    "\n",
    "N = 10  # number of iterations\n",
    "\n",
    "# optimization\n",
    "for k in range(N):\n",
    "\n",
    "    # sets all gradients to zero (this is important)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # compute the quadratic function\n",
    "    f = 3 * x**2 - 4 * x + 5\n",
    "\n",
    "    f.backward()\n",
    "\n",
    "    # update x (sgd)\n",
    "    optimizer.step()\n",
    "    # the same as\n",
    "    # x -= learning_rate * x.grad.item() # NOTE: does not run!\n",
    "\n",
    "    print(f\"step {k}, gradient x = {x.grad.item()}\")\n",
    "\n",
    "print(f\"---\")\n",
    "print(f\"x' reached: {x.data.item():.3f} after {N} iterations and should be {2/3:.3f}!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recap: Naive Line Fitting (from Tutorial 07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a simple example of line fitting, where we try to fit a line with a simple line equation: $y = mx + b$.\n",
    "We use the `scipy.optimize` package to fit the line to the data. <br>\n",
    "Note that this will only work for a single line and breaks if there are multiple lines or noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "image = np.zeros((50, 50), dtype=np.uint8)\n",
    "image[3:33, 10:40] = np.eye(30) * 255\n",
    "# let's add random noise (off if N=0)\n",
    "N = 0\n",
    "image[np.random.randint(0, 50, N), np.random.randint(0, 50, N)] = 255\n",
    "\n",
    "\n",
    "# get all the non-zero points\n",
    "points = np.argwhere(image)\n",
    "ys, xs = points[:, 0], points[:, 1]\n",
    "\n",
    "# a simple line equation y = mx + b (m is the slope, which you might also know as k)\n",
    "def line_eq(x, m, b):\n",
    "    return m * x + b\n",
    "\n",
    "\n",
    "# find m and b\n",
    "(m, b), _ = curve_fit(line_eq, xs, ys)\n",
    "print(m, b)\n",
    "\n",
    "\n",
    "# yshat = line_eq(xs, m, b)\n",
    "\n",
    "# show\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "# plt.plot(xs, line_eq(xs, m, b))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's solve it with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Convert numpy array to torch tensors\n",
    "_xs = torch.from_numpy(xs).float()\n",
    "_ys = torch.from_numpy(ys).float()\n",
    "\n",
    "# define the model parameters (m and b)\n",
    "_m = torch.Tensor([0.1]).float()\n",
    "_m.requires_grad = True\n",
    "_b = torch.Tensor([0]).float()\n",
    "_b.requires_grad = True\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.Adam([_m, _b], lr=1e-1)\n",
    "\n",
    "print(f\"The initial model parameters are: m={_m.item():.3f}, b={_b.item():.3f}\")\n",
    "yhat = line_eq(_xs, _m, _b)\n",
    "loss = criterion(yhat, _ys)\n",
    "print(f\"The initial loss is: {loss.item():.3f}\")\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    yhat = line_eq(_xs, _m, _b)\n",
    "    loss = criterion(yhat, _ys)\n",
    "    loss.backward()  # compute gradients\n",
    "    optimizer.step()  # update parameters (m and b)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"step {epoch}, loss = {loss.item():.3f}\")\n",
    "\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "yhat = line_eq(_xs, _m, _b)\n",
    "plt.plot(xs, yhat.detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "print(f\"The final model parameters are: m={_m.item():.3f}, b={_b.item():.3f}\")\n",
    "print(f\"The final loss is: {loss.item():.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RZFroauemFF-"
   },
   "source": [
    "### Exercise 1 📝: Play with the hyper parameters\n",
    " \n",
    "Change the initial parameters ($m,b$) and the hyperparameters (learning rate and epochs) and see how it affects traininig. \n",
    "\n",
    "\n",
    "*Advanced:* Try to change the optimizer (see the [PyTorch docs](https://pytorch.org/docs/stable/optim.html#algorithms)) and see how it affects the training. For example, try to use the `SGD` optimizer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "W3ttSd1AmFF8"
   },
   "source": [
    "# Let's train a linear classifier to classify 2D data points. \n",
    "\n",
    "We will at first generate some data using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "5Juvwq-QmFF8",
    "outputId": "1898ae0a-d731-4bbd-cd9c-50a922353fa7"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_blobs, make_circles\n",
    "\n",
    "# Set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "num_classes = 2\n",
    "N = 100  # number of points\n",
    "\n",
    "# make up a dataset\n",
    "X, y = make_blobs(n_samples=N, centers=num_classes)  # , noise=0.1)\n",
    "# TODO: use later -> X, y = make_moons(n_samples=N, noise=0.1)\n",
    "\n",
    "# y = y * 2 - 1  # make y be -1 or 1\n",
    "# visualize in 2D\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=plt.cm.Spectral)\n",
    "plt.show()\n",
    "print(np.unique(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our Linear Classifier in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the number of inputs and outputs\n",
    "num_inputs = 2  # dimension (x, y)\n",
    "num_outputs = num_classes\n",
    "\n",
    "# Define the input and label tensors\n",
    "inputs = torch.tensor(X, dtype=torch.float32)\n",
    "labels = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Define the model\n",
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = LinearClassifier()\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 02 📝: Train the model\n",
    " \n",
    "- Train our linear classifier. Use the optimizer to update the model parameters. Try to answer the following questions:\n",
    "    - How many epochs do you need to train the model? \n",
    "    - How do the loss and accuracy change over time?\n",
    "- Plot the decision boundary of the model (below).\n",
    "\n",
    "### Exercise 03 📝: Change the data generation function\n",
    "- Can you still classify the points if you switch to the following data generation function?\n",
    "    ```python \n",
    "    X, y = make_moons(n_samples=N, noise=0.1)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(inputs)\n",
    "_, preds = logits.max(1)\n",
    "acc = (preds == labels).float().mean()\n",
    "print(f\"Initial (random) Accuracy: {acc.item():.3f}\")\n",
    "\n",
    "\n",
    "# Todo: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize decision boundary (similar to tensorflow playground)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define a function to plot the decision boundaries\n",
    "def plot_decision_boundary(model):\n",
    "    x_min, x_max = inputs[:, 0].min() - 0.1, inputs[:, 0].max() + 0.1\n",
    "    y_min, y_max = inputs[:, 1].min() - 0.1, inputs[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "    logits = model(torch.from_numpy(Xmesh).float())\n",
    "    _, Z = logits.max(1)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.2)\n",
    "\n",
    "\n",
    "# Plot the points and decision boundaries\n",
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=labels, cmap=plt.cm.Spectral)\n",
    "plot_decision_boundary(model)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a05e4fa746b81761c76a645b508c0f51cdd970f4b4b50ae36c6a73f9a174174"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

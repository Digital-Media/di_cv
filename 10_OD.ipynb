{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Tutorial 10 - Object Detection with YOLO on our Dataset\n",
    "\n",
    "## Dr. David C. Schedl\n",
    "\n",
    "## Setup\n",
    "As first step, we need to import the necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset extracted to data/ directory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!curl -LJO \"https://raw.githubusercontent.com/Digital-Media/cv_data/main/fhhgb-hockey-dataset.zip\" --silent\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Extract the dataset to the data directory\n",
    "with zipfile.ZipFile(\"fhhgb-hockey-dataset.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"data\")\n",
    "    \n",
    "print(\"Dataset extracted to data/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Loading a Pre-trained Model\n",
    "\n",
    "We'll start by loading a pre-trained YOLO model that has been trained on the FHHGB Hockey Dataset. This model can detect hockey nets and pucks in hockey game footage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Model classes: {0: 'net', 1: 'puck'}\n"
     ]
    }
   ],
   "source": [
    "# Setup and import of libraries\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Load a pretrained YOLO model trained on hockey dataset\n",
    "model = YOLO(r\"data\\FHHGB-Hockey-Dataset\\models\\weights.pt\")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model classes: {model.names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Testing on a Single Image\n",
    "\n",
    "Let's test our model on a single image from the test set to see how it performs. We'll use one of the hockey images to demonstrate the object detection capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using test image: data/FHHGB-Hockey-Dataset/test/images/data1-12_png_jpg.rf.ec74890a35a41e81fa73e82288f71d83.jpg\n",
      "\n",
      "image 1/1 c:\\Users\\P22598\\Github\\di_cv\\data\\FHHGB-Hockey-Dataset\\test\\images\\data1-12_png_jpg.rf.ec74890a35a41e81fa73e82288f71d83.jpg: 1024x1024 1 puck, 242.7ms\n",
      "Speed: 10.6ms preprocess, 242.7ms inference, 3.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "Detection completed on: data/FHHGB-Hockey-Dataset/test/images/data1-12_png_jpg.rf.ec74890a35a41e81fa73e82288f71d83.jpg\n"
     ]
    }
   ],
   "source": [
    "# Select a test image from the hockey dataset\n",
    "test_image_path = \"data/FHHGB-Hockey-Dataset/test/images/data1-12_png_jpg.rf.ec74890a35a41e81fa73e82288f71d83.jpg\"\n",
    "\n",
    "# Verify the image exists\n",
    "if os.path.exists(test_image_path):\n",
    "    print(f\"Using test image: {test_image_path}\")\n",
    "else:\n",
    "    print(\"Test image not found, using a sample image instead\")\n",
    "    test_image_path = \"bus.jpg\"  # Fallback to the bus image\n",
    "\n",
    "# Perform object detection on the test image\n",
    "results = model(test_image_path)\n",
    "\n",
    "print(f\"Detection completed on: {test_image_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Visualizing Results\n",
    "\n",
    "Now let's visualize the detection results. The model will draw bounding boxes around detected objects and label them with their class names and confidence scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved as: results0.jpg\n"
     ]
    }
   ],
   "source": [
    "# Visualize the results\n",
    "for i, r in enumerate(results):\n",
    "    # Plot results image\n",
    "    im_bgr = r.plot()  # BGR-order numpy array\n",
    "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
    "    \n",
    "    # Display the image with detections\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(im_rgb)\n",
    "    plt.title(f\"Object Detection Results - Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show results to screen (in supported environments)\n",
    "    # r.show()\n",
    "    \n",
    "    # Save results to disk\n",
    "    r.save(filename=f\"results{i}.jpg\")\n",
    "    print(f\"Results saved as: results{i}.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Analyzing Detection Results\n",
    "\n",
    "Let's examine the detection results in more detail to understand what objects were detected and their confidence scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Detection Results for Image 1 ===\n",
      "Number of detections: 1\n",
      "  Detection 1: puck (confidence: 0.571)\n",
      "    Bounding box: (480.8, 615.7) to (500.2, 629.3)\n"
     ]
    }
   ],
   "source": [
    "# Analyze detection results\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"\\n=== Detection Results for Image {i+1} ===\")\n",
    "    \n",
    "    if r.boxes is not None:\n",
    "        # Get detection information\n",
    "        boxes = r.boxes\n",
    "        \n",
    "        print(f\"Number of detections: {len(boxes)}\")\n",
    "        \n",
    "        for j, box in enumerate(boxes):\n",
    "            # Get class name and confidence\n",
    "            cls = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "            \n",
    "            # Get class name from model names\n",
    "            class_name = model.names[cls]\n",
    "            \n",
    "            # Get bounding box coordinates\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            \n",
    "            print(f\"  Detection {j+1}: {class_name} (confidence: {conf:.3f})\")\n",
    "            print(f\"    Bounding box: ({x1:.1f}, {y1:.1f}) to ({x2:.1f}, {y2:.1f})\")\n",
    "    else:\n",
    "        print(\"No objects detected in this image.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Model Validation\n",
    "\n",
    "Now let's evaluate the model's performance on the validation set to get quantitative metrics like precision, recall, and mAP (mean Average Precision).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation on the validation set...\n",
      "Ultralytics 8.3.156  Python-3.12.6 torch-2.7.1+cpu CPU (Intel Core(TM) i7-10750H 2.60GHz)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.50.1 ms, read: 50.620.5 MB/s, size: 60.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\P22598\\Github\\di_cv\\data\\FHHGB-Hockey-Dataset\\valid\\labels... 99 images, 5 backgrounds, 0 corrupt: 100%|██████████| 99/99 [00:00<00:00, 208.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\P22598\\Github\\di_cv\\data\\FHHGB-Hockey-Dataset\\valid\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\P22598\\Github\\di_cv\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 7/7 [00:36<00:00,  5.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         99        154      0.717      0.644      0.646      0.473\n",
      "                   net         62         62      0.924      0.952      0.976      0.838\n",
      "                  puck         91         92       0.51      0.337      0.316      0.109\n",
      "Speed: 10.2ms preprocess, 327.7ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mc:\\Users\\P22598\\Github\\di_cv\\runs\\detect\\val4\u001b[0m\n",
      "\n",
      "=== Validation Results ===\n",
      "Overall mAP@0.5: 0.473\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Metric' object has no attribute 'map50_95'. See valid attributes below.\n\n    Class for computing evaluation metrics for Ultralytics YOLO models.\n\n    Attributes:\n        p (list): Precision for each class. Shape: (nc,).\n        r (list): Recall for each class. Shape: (nc,).\n        f1 (list): F1 score for each class. Shape: (nc,).\n        all_ap (list): AP scores for all classes and all IoU thresholds. Shape: (nc, 10).\n        ap_class_index (list): Index of class for each AP score. Shape: (nc,).\n        nc (int): Number of classes.\n\n    Methods:\n        ap50(): AP at IoU threshold of 0.5 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n        ap(): AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n        mp(): Mean precision of all classes. Returns: Float.\n        mr(): Mean recall of all classes. Returns: Float.\n        map50(): Mean AP at IoU threshold of 0.5 for all classes. Returns: Float.\n        map75(): Mean AP at IoU threshold of 0.75 for all classes. Returns: Float.\n        map(): Mean AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: Float.\n        mean_results(): Mean of results, returns mp, mr, map50, map.\n        class_result(i): Class-aware result, returns p[i], r[i], ap50[i], ap[i].\n        maps(): mAP of each class. Returns: Array of mAP scores, shape: (nc,).\n        fitness(): Model fitness as a weighted combination of metrics. Returns: Float.\n        update(results): Update metric attributes with new evaluation results.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Validation Results ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOverall mAP@0.5: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_val.box.map\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOverall mAP@0.5:0.95: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mmetrics_val\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap50_95\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOverall Precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_val.box.mp\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOverall Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics_val.box.mr\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\P22598\\Github\\di_cv\\venv\\Lib\\site-packages\\ultralytics\\utils\\__init__.py:394\u001b[39m, in \u001b[36mSimpleClass.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    392\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Provide a custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[32m    393\u001b[39m name = \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Metric' object has no attribute 'map50_95'. See valid attributes below.\n\n    Class for computing evaluation metrics for Ultralytics YOLO models.\n\n    Attributes:\n        p (list): Precision for each class. Shape: (nc,).\n        r (list): Recall for each class. Shape: (nc,).\n        f1 (list): F1 score for each class. Shape: (nc,).\n        all_ap (list): AP scores for all classes and all IoU thresholds. Shape: (nc, 10).\n        ap_class_index (list): Index of class for each AP score. Shape: (nc,).\n        nc (int): Number of classes.\n\n    Methods:\n        ap50(): AP at IoU threshold of 0.5 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n        ap(): AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: List of AP scores. Shape: (nc,) or [].\n        mp(): Mean precision of all classes. Returns: Float.\n        mr(): Mean recall of all classes. Returns: Float.\n        map50(): Mean AP at IoU threshold of 0.5 for all classes. Returns: Float.\n        map75(): Mean AP at IoU threshold of 0.75 for all classes. Returns: Float.\n        map(): Mean AP at IoU thresholds from 0.5 to 0.95 for all classes. Returns: Float.\n        mean_results(): Mean of results, returns mp, mr, map50, map.\n        class_result(i): Class-aware result, returns p[i], r[i], ap50[i], ap[i].\n        maps(): mAP of each class. Returns: Array of mAP scores, shape: (nc,).\n        fitness(): Model fitness as a weighted combination of metrics. Returns: Float.\n        update(results): Update metric attributes with new evaluation results.\n    "
     ]
    }
   ],
   "source": [
    "# Validate on validation set\n",
    "print(\"Running validation on the validation set...\")\n",
    "metrics_val = model.val(data=\"data/FHHGB-Hockey-Dataset/data.yaml\", split=\"val\")\n",
    "\n",
    "print(\"\\n=== Validation Results ===\")\n",
    "print(f\"Overall mAP@0.5: {metrics_val.box.map:.3f}\")\n",
    "print(f\"Overall mAP@0.5:0.95: {metrics_val.box.map50_95:.3f}\")\n",
    "print(f\"Overall Precision: {metrics_val.box.mp:.3f}\")\n",
    "print(f\"Overall Recall: {metrics_val.box.mr:.3f}\")\n",
    "\n",
    "print(\"\\nResults saved to the 'runs/detect/' directory\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Understanding the Results\n",
    "\n",
    "## Performance Metrics Explained\n",
    "\n",
    "- **mAP@0.5**: Mean Average Precision at IoU threshold of 0.5\n",
    "- **mAP@0.5:0.95**: Mean Average Precision averaged over IoU thresholds from 0.5 to 0.95\n",
    "- **Precision**: Ratio of true positive detections to all positive detections\n",
    "- **Recall**: Ratio of true positive detections to all actual objects\n",
    "\n",
    "## Class-Specific Performance\n",
    "\n",
    "The model shows different performance for different classes:\n",
    "- **Net detection**: Generally high accuracy due to its large, distinctive appearance\n",
    "- **Puck detection**: Lower accuracy due to its small size and fast movement\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "This hockey object detection model could be used for:\n",
    "- Automated game analysis\n",
    "- Player tracking systems\n",
    "- Goal detection and verification\n",
    "- Training data generation for sports analytics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 📝 Exercise\n",
    "\n",
    "Try the following exercises to better understand object detection:\n",
    "\n",
    "1. **Test on different images**: Try running the model on other test images from the dataset\n",
    "2. **Analyze failure cases**: Look for images where the model fails to detect objects correctly\n",
    "3. **Confidence thresholding**: Experiment with different confidence thresholds to see how they affect detection results\n",
    "4. **Real-time detection**: If you have a webcam, try running the model on live video feed\n",
    "\n",
    "## Useful Links\n",
    "\n",
    "- [Ultralytics YOLO Documentation](https://docs.ultralytics.com/)\n",
    "- [YOLO Paper](https://arxiv.org/abs/1506.02640)\n",
    "- [Object Detection Tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
